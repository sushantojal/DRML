{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rr_8_8\n",
      "Tensor(\"Relu_64:0\", shape=(10, 20, 20, 32), dtype=float32)\n",
      "rout_1_1\n",
      "Tensor(\"Add:0\", shape=(10, 20, 20, 32), dtype=float32)\n",
      "r1\n",
      "Tensor(\"concat:0\", shape=(10, 20, 160, 32), dtype=float32)\n",
      "rtotal\n",
      "Tensor(\"concat_8:0\", shape=(10, 160, 160, 32), dtype=float32)\n",
      "mpool\n",
      "Tensor(\"MaxPool:0\", shape=(10, 80, 80, 32), dtype=float32)\n",
      "Tensor(\"Relu_71:0\", shape=(10, 27, 27, 16), dtype=float32)\n",
      "Tensor(\"dropout/mul_1:0\", shape=(10, 4096), dtype=float32)\n",
      "Tensor(\"dropout_1/mul_1:0\", shape=(10, 2048), dtype=float32)\n",
      "Tensor(\"Add_66:0\", shape=(10, 2), dtype=float32)\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "Iter 50, Minibatch Loss= 3751147995136.000000, Training Accuracy= 0.50000\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "# from PIL import Image, ImageFile\n",
    "import matplotlib.image as mpimg\n",
    "from numpy import genfromtxt\n",
    "from numpy.random import permutation\n",
    "\n",
    "#read data into memory\n",
    "my_data = genfromtxt('/root/mydataset_higher_5_6', delimiter=',', dtype=None)\n",
    "batch_y_t = np.zeros([len(my_data), 2])\n",
    "batch_x_t = []\n",
    "index  = 0\n",
    "for a in my_data:\n",
    "    if a[0] == 5:\n",
    "        batch_y_t[index][0] = 1\n",
    "    else:\n",
    "        batch_y_t[index][1] = 1\n",
    "    batch_x_t.append(a[1])\n",
    "    index = index+1\n",
    "    \n",
    "batch_x_t = np.asarray(batch_x_t)\n",
    "\n",
    "\n",
    "\n",
    "def get_next_batch(start_i, end_i):\n",
    "    btc = []\n",
    "    for i in range(start_i, end_i+1):\n",
    "        global batch_x_t\n",
    "        global batch_y_t\n",
    "        img = mpimg.imread('/root/dataset_png/'+batch_x_t[i])\n",
    "#         print(img.shape)\n",
    "        greyscale_map = img.flatten()\n",
    "        greyscale_map = np.array(greyscale_map)\n",
    "        btc.append(greyscale_map.ravel())\n",
    "    return np.asarray(btc).astype('float32'), batch_y_t[start_i:end_i+1]\n",
    "\n",
    "\n",
    "\n",
    "#Permute data\n",
    "p = permutation(len(batch_y_t))\n",
    "batch_x_t = batch_x_t[p]\n",
    "batch_y_t = batch_y_t[p]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 170*170*3\n",
    "n_classes = 2 #  total classes currently we will only take 2\n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "display_step = 5\n",
    "\n",
    "\n",
    "training_iters  = 1200\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 170 * 170 *3])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1, pad='VALID'):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=pad)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    pp = tf.nn.relu(x)\n",
    "    return pp\n",
    "\n",
    "def conv2d_without_bias(x, W, strides=1, pad='SAME'):\n",
    "    # Conv2D wrapper, without bias and without relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=pad)\n",
    "    return x\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    \n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 170, 170, 3])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'], 1, 'VALID')\n",
    "    \n",
    "    #REGION LAYER HERE\n",
    "    r_1_1 = tf.slice(conv1, [0,0,0,0], [10, 20, 20, 32])\n",
    "    r_1_2 = tf.slice(conv1, [0,0,20,0], [10, 20, 20, 32])\n",
    "    r_1_3 = tf.slice(conv1, [0,0,40,0], [10, 20, 20, 32])\n",
    "    r_1_4 = tf.slice(conv1, [0,0,60,0], [10, 20, 20, 32])\n",
    "    r_1_5 = tf.slice(conv1, [0,0,80,0], [10, 20, 20, 32])\n",
    "    r_1_6 = tf.slice(conv1, [0,0,100,0], [10, 20, 20, 32])\n",
    "    r_1_7 = tf.slice(conv1, [0,0,120,0], [10, 20, 20, 32])\n",
    "    r_1_8 = tf.slice(conv1, [0,0,140,0], [10, 20, 20, 32])\n",
    "    r_2_1 = tf.slice(conv1, [0,20,0,0], [10, 20, 20, 32])\n",
    "    r_2_2 = tf.slice(conv1, [0,20,20,0], [10, 20, 20, 32])\n",
    "    r_2_3 = tf.slice(conv1, [0,20,40,0], [10, 20, 20, 32])\n",
    "    r_2_4 = tf.slice(conv1, [0,20,60,0], [10, 20, 20, 32])\n",
    "    r_2_5 = tf.slice(conv1, [0,20,80,0], [10, 20, 20, 32])\n",
    "    r_2_6 = tf.slice(conv1, [0,20,100,0], [10, 20, 20, 32])\n",
    "    r_2_7 = tf.slice(conv1, [0,20,120,0], [10, 20, 20, 32])\n",
    "    r_2_8 = tf.slice(conv1, [0,20,140,0], [10, 20, 20, 32])\n",
    "    r_3_1 = tf.slice(conv1, [0,40,0,0], [10, 20, 20, 32])\n",
    "    r_3_2 = tf.slice(conv1, [0,40,20,0], [10, 20, 20, 32])\n",
    "    r_3_3 = tf.slice(conv1, [0,40,40,0], [10, 20, 20, 32])\n",
    "    r_3_4 = tf.slice(conv1, [0,40,60,0], [10, 20, 20, 32])\n",
    "    r_3_5 = tf.slice(conv1, [0,40,80,0], [10, 20, 20, 32])\n",
    "    r_3_6 = tf.slice(conv1, [0,40,100,0], [10, 20, 20, 32])\n",
    "    r_3_7 = tf.slice(conv1, [0,40,120,0], [10, 20, 20, 32])\n",
    "    r_3_8 = tf.slice(conv1, [0,40,140,0], [10, 20, 20, 32])\n",
    "    r_4_1 = tf.slice(conv1, [0,60,0,0], [10, 20, 20, 32])\n",
    "    r_4_2 = tf.slice(conv1, [0,60,20,0], [10, 20, 20, 32])\n",
    "    r_4_3 = tf.slice(conv1, [0,60,40,0], [10, 20, 20, 32])\n",
    "    r_4_4 = tf.slice(conv1, [0,60,60,0], [10, 20, 20, 32])\n",
    "    r_4_5 = tf.slice(conv1, [0,60,80,0], [10, 20, 20, 32])\n",
    "    r_4_6 = tf.slice(conv1, [0,60,100,0], [10, 20, 20, 32])\n",
    "    r_4_7 = tf.slice(conv1, [0,60,120,0], [10, 20, 20, 32])\n",
    "    r_4_8 = tf.slice(conv1, [0,60,140,0], [10, 20, 20, 32])\n",
    "    r_5_1 = tf.slice(conv1, [0,80,0,0], [10, 20, 20, 32])\n",
    "    r_5_2 = tf.slice(conv1, [0,80,20,0], [10, 20, 20, 32])\n",
    "    r_5_3 = tf.slice(conv1, [0,80,40,0], [10, 20, 20, 32])\n",
    "    r_5_4 = tf.slice(conv1, [0,80,60,0], [10, 20, 20, 32])\n",
    "    r_5_5 = tf.slice(conv1, [0,80,80,0], [10, 20, 20, 32])\n",
    "    r_5_6 = tf.slice(conv1, [0,80,100,0], [10, 20, 20, 32])\n",
    "    r_5_7 = tf.slice(conv1, [0,80,120,0], [10, 20, 20, 32])\n",
    "    r_5_8 = tf.slice(conv1, [0,80,140,0], [10, 20, 20, 32])\n",
    "    r_6_1 = tf.slice(conv1, [0,100,0,0], [10, 20, 20, 32])\n",
    "    r_6_2 = tf.slice(conv1, [0,100,20,0], [10, 20, 20, 32])\n",
    "    r_6_3 = tf.slice(conv1, [0,100,40,0], [10, 20, 20, 32])\n",
    "    r_6_4 = tf.slice(conv1, [0,100,60,0], [10, 20, 20, 32])\n",
    "    r_6_5 = tf.slice(conv1, [0,100,80,0], [10, 20, 20, 32])\n",
    "    r_6_6 = tf.slice(conv1, [0,100,100,0], [10, 20, 20, 32])\n",
    "    r_6_7 = tf.slice(conv1, [0,100,120,0], [10, 20, 20, 32])\n",
    "    r_6_8 = tf.slice(conv1, [0,100,140,0], [10, 20, 20, 32])\n",
    "    r_7_1 = tf.slice(conv1, [0,120,0,0], [10, 20, 20, 32])\n",
    "    r_7_2 = tf.slice(conv1, [0,120,20,0], [10, 20, 20, 32])\n",
    "    r_7_3 = tf.slice(conv1, [0,120,40,0], [10, 20, 20, 32])\n",
    "    r_7_4 = tf.slice(conv1, [0,120,60,0], [10, 20, 20, 32])\n",
    "    r_7_5 = tf.slice(conv1, [0,120,80,0], [10, 20, 20, 32])\n",
    "    r_7_6 = tf.slice(conv1, [0,120,100,0], [10, 20, 20, 32])\n",
    "    r_7_7 = tf.slice(conv1, [0,120,120,0], [10, 20, 20, 32])\n",
    "    r_7_8 = tf.slice(conv1, [0,120,140,0], [10, 20, 20, 32])\n",
    "    r_8_1 = tf.slice(conv1, [0,140,0,0], [10, 20, 20, 32])\n",
    "    r_8_2 = tf.slice(conv1, [0,140,20,0], [10, 20, 20, 32])\n",
    "    r_8_3 = tf.slice(conv1, [0,140,40,0], [10, 20, 20, 32])\n",
    "    r_8_4 = tf.slice(conv1, [0,140,60,0], [10, 20, 20, 32])\n",
    "    r_8_5 = tf.slice(conv1, [0,140,80,0], [10, 20, 20, 32])\n",
    "    r_8_6 = tf.slice(conv1, [0,140,100,0], [10, 20, 20, 32])\n",
    "    r_8_7 = tf.slice(conv1, [0,140,120,0], [10, 20, 20, 32])\n",
    "    r_8_8 = tf.slice(conv1, [0,140,140,0], [10, 20, 20, 32])\n",
    "    \n",
    "    #REGION LAYER Local relu\n",
    "    rr_1_1 = tf.nn.relu(r_1_1)\n",
    "    rr_1_2 = tf.nn.relu(r_1_2)\n",
    "    rr_1_3 = tf.nn.relu(r_1_3)\n",
    "    rr_1_4 = tf.nn.relu(r_1_4)\n",
    "    rr_1_5 = tf.nn.relu(r_1_5)\n",
    "    rr_1_6 = tf.nn.relu(r_1_6)\n",
    "    rr_1_7 = tf.nn.relu(r_1_7)\n",
    "    rr_1_8 = tf.nn.relu(r_1_8)\n",
    "    rr_2_1 = tf.nn.relu(r_2_1)\n",
    "    rr_2_2 = tf.nn.relu(r_2_2)\n",
    "    rr_2_3 = tf.nn.relu(r_2_3)\n",
    "    rr_2_4 = tf.nn.relu(r_2_4)\n",
    "    rr_2_5 = tf.nn.relu(r_2_5)\n",
    "    rr_2_6 = tf.nn.relu(r_2_6)\n",
    "    rr_2_7 = tf.nn.relu(r_2_7)\n",
    "    rr_2_8 = tf.nn.relu(r_2_8)\n",
    "    rr_3_1 = tf.nn.relu(r_3_1)\n",
    "    rr_3_2 = tf.nn.relu(r_3_2)\n",
    "    rr_3_3 = tf.nn.relu(r_3_3)\n",
    "    rr_3_4 = tf.nn.relu(r_3_4)\n",
    "    rr_3_5 = tf.nn.relu(r_3_5)\n",
    "    rr_3_6 = tf.nn.relu(r_3_6)\n",
    "    rr_3_7 = tf.nn.relu(r_3_7)\n",
    "    rr_3_8 = tf.nn.relu(r_3_8)\n",
    "    rr_4_1 = tf.nn.relu(r_4_1)\n",
    "    rr_4_2 = tf.nn.relu(r_4_2)\n",
    "    rr_4_3 = tf.nn.relu(r_4_3)\n",
    "    rr_4_4 = tf.nn.relu(r_4_4)\n",
    "    rr_4_5 = tf.nn.relu(r_4_5)\n",
    "    rr_4_6 = tf.nn.relu(r_4_6)\n",
    "    rr_4_7 = tf.nn.relu(r_4_7)\n",
    "    rr_4_8 = tf.nn.relu(r_4_8)\n",
    "    rr_5_1 = tf.nn.relu(r_5_1)\n",
    "    rr_5_2 = tf.nn.relu(r_5_2)\n",
    "    rr_5_3 = tf.nn.relu(r_5_3)\n",
    "    rr_5_4 = tf.nn.relu(r_5_4)\n",
    "    rr_5_5 = tf.nn.relu(r_5_5)\n",
    "    rr_5_6 = tf.nn.relu(r_5_6)\n",
    "    rr_5_7 = tf.nn.relu(r_5_7)\n",
    "    rr_5_8 = tf.nn.relu(r_5_8)\n",
    "    rr_6_1 = tf.nn.relu(r_6_1)\n",
    "    rr_6_2 = tf.nn.relu(r_6_2)\n",
    "    rr_6_3 = tf.nn.relu(r_6_3)\n",
    "    rr_6_4 = tf.nn.relu(r_6_4)\n",
    "    rr_6_5 = tf.nn.relu(r_6_5)\n",
    "    rr_6_6 = tf.nn.relu(r_6_6)\n",
    "    rr_6_7 = tf.nn.relu(r_6_7)\n",
    "    rr_6_8 = tf.nn.relu(r_6_8)\n",
    "    rr_7_1 = tf.nn.relu(r_7_1)\n",
    "    rr_7_2 = tf.nn.relu(r_7_2)\n",
    "    rr_7_3 = tf.nn.relu(r_7_3)\n",
    "    rr_7_4 = tf.nn.relu(r_7_4)\n",
    "    rr_7_5 = tf.nn.relu(r_7_5)\n",
    "    rr_7_6 = tf.nn.relu(r_7_6)\n",
    "    rr_7_7 = tf.nn.relu(r_7_7)\n",
    "    rr_7_8 = tf.nn.relu(r_7_8)\n",
    "    rr_8_1 = tf.nn.relu(r_8_1)\n",
    "    rr_8_2 = tf.nn.relu(r_8_2)\n",
    "    rr_8_3 = tf.nn.relu(r_8_3)\n",
    "    rr_8_4 = tf.nn.relu(r_8_4)\n",
    "    rr_8_5 = tf.nn.relu(r_8_5)\n",
    "    rr_8_6 = tf.nn.relu(r_8_6)\n",
    "    rr_8_7 = tf.nn.relu(r_8_7)\n",
    "    rr_8_8 = tf.nn.relu(r_8_8)\n",
    "    \n",
    "    print('rr_8_8')\n",
    "    print(rr_8_8)\n",
    "    rcv_1_1 = conv2d_without_bias(rr_1_1, weights['wc_1_1'])\n",
    "    rcv_1_2 = conv2d_without_bias(rr_1_2, weights['wc_1_2'])\n",
    "    rcv_1_3 = conv2d_without_bias(rr_1_3, weights['wc_1_3'])\n",
    "    rcv_1_4 = conv2d_without_bias(rr_1_4, weights['wc_1_4'])\n",
    "    rcv_1_5 = conv2d_without_bias(rr_1_5, weights['wc_1_5'])\n",
    "    rcv_1_6 = conv2d_without_bias(rr_1_6, weights['wc_1_6'])\n",
    "    rcv_1_7 = conv2d_without_bias(rr_1_7, weights['wc_1_7'])\n",
    "    rcv_1_8 = conv2d_without_bias(rr_1_8, weights['wc_1_8'])\n",
    "    rcv_2_1 = conv2d_without_bias(rr_2_1, weights['wc_2_1'])\n",
    "    rcv_2_2 = conv2d_without_bias(rr_2_2, weights['wc_2_2'])\n",
    "    rcv_2_3 = conv2d_without_bias(rr_2_3, weights['wc_2_3'])\n",
    "    rcv_2_4 = conv2d_without_bias(rr_2_4, weights['wc_2_4'])\n",
    "    rcv_2_5 = conv2d_without_bias(rr_2_5, weights['wc_2_5'])\n",
    "    rcv_2_6 = conv2d_without_bias(rr_2_6, weights['wc_2_6'])\n",
    "    rcv_2_7 = conv2d_without_bias(rr_2_7, weights['wc_2_7'])\n",
    "    rcv_2_8 = conv2d_without_bias(rr_2_8, weights['wc_2_8'])\n",
    "    rcv_3_1 = conv2d_without_bias(rr_3_1, weights['wc_3_1'])\n",
    "    rcv_3_2 = conv2d_without_bias(rr_3_2, weights['wc_3_2'])\n",
    "    rcv_3_3 = conv2d_without_bias(rr_3_3, weights['wc_3_3'])\n",
    "    rcv_3_4 = conv2d_without_bias(rr_3_4, weights['wc_3_4'])\n",
    "    rcv_3_5 = conv2d_without_bias(rr_3_5, weights['wc_3_5'])\n",
    "    rcv_3_6 = conv2d_without_bias(rr_3_6, weights['wc_3_6'])\n",
    "    rcv_3_7 = conv2d_without_bias(rr_3_7, weights['wc_3_7'])\n",
    "    rcv_3_8 = conv2d_without_bias(rr_3_8, weights['wc_3_8'])\n",
    "    rcv_4_1 = conv2d_without_bias(rr_4_1, weights['wc_4_1'])\n",
    "    rcv_4_2 = conv2d_without_bias(rr_4_2, weights['wc_4_2'])\n",
    "    rcv_4_3 = conv2d_without_bias(rr_4_3, weights['wc_4_3'])\n",
    "    rcv_4_4 = conv2d_without_bias(rr_4_4, weights['wc_4_4'])\n",
    "    rcv_4_5 = conv2d_without_bias(rr_4_5, weights['wc_4_5'])\n",
    "    rcv_4_6 = conv2d_without_bias(rr_4_6, weights['wc_4_6'])\n",
    "    rcv_4_7 = conv2d_without_bias(rr_4_7, weights['wc_4_7'])\n",
    "    rcv_4_8 = conv2d_without_bias(rr_4_8, weights['wc_4_8'])\n",
    "    rcv_5_1 = conv2d_without_bias(rr_5_1, weights['wc_5_1'])\n",
    "    rcv_5_2 = conv2d_without_bias(rr_5_2, weights['wc_5_2'])\n",
    "    rcv_5_3 = conv2d_without_bias(rr_5_3, weights['wc_5_3'])\n",
    "    rcv_5_4 = conv2d_without_bias(rr_5_4, weights['wc_5_4'])\n",
    "    rcv_5_5 = conv2d_without_bias(rr_5_5, weights['wc_5_5'])\n",
    "    rcv_5_6 = conv2d_without_bias(rr_5_6, weights['wc_5_6'])\n",
    "    rcv_5_7 = conv2d_without_bias(rr_5_7, weights['wc_5_7'])\n",
    "    rcv_5_8 = conv2d_without_bias(rr_5_8, weights['wc_5_8'])\n",
    "    rcv_6_1 = conv2d_without_bias(rr_6_1, weights['wc_6_1'])\n",
    "    rcv_6_2 = conv2d_without_bias(rr_6_2, weights['wc_6_2'])\n",
    "    rcv_6_3 = conv2d_without_bias(rr_6_3, weights['wc_6_3'])\n",
    "    rcv_6_4 = conv2d_without_bias(rr_6_4, weights['wc_6_4'])\n",
    "    rcv_6_5 = conv2d_without_bias(rr_6_5, weights['wc_6_5'])\n",
    "    rcv_6_6 = conv2d_without_bias(rr_6_6, weights['wc_6_6'])\n",
    "    rcv_6_7 = conv2d_without_bias(rr_6_7, weights['wc_6_7'])\n",
    "    rcv_6_8 = conv2d_without_bias(rr_6_8, weights['wc_6_8'])\n",
    "    rcv_7_1 = conv2d_without_bias(rr_7_1, weights['wc_7_1'])\n",
    "    rcv_7_2 = conv2d_without_bias(rr_7_2, weights['wc_7_2'])\n",
    "    rcv_7_3 = conv2d_without_bias(rr_7_3, weights['wc_7_3'])\n",
    "    rcv_7_4 = conv2d_without_bias(rr_7_4, weights['wc_7_4'])\n",
    "    rcv_7_5 = conv2d_without_bias(rr_7_5, weights['wc_7_5'])\n",
    "    rcv_7_6 = conv2d_without_bias(rr_7_6, weights['wc_7_6'])\n",
    "    rcv_7_7 = conv2d_without_bias(rr_7_7, weights['wc_7_7'])\n",
    "    rcv_7_8 = conv2d_without_bias(rr_7_8, weights['wc_7_8'])\n",
    "    rcv_8_1 = conv2d_without_bias(rr_8_1, weights['wc_8_1'])\n",
    "    rcv_8_2 = conv2d_without_bias(rr_8_2, weights['wc_8_2'])\n",
    "    rcv_8_3 = conv2d_without_bias(rr_8_3, weights['wc_8_3'])\n",
    "    rcv_8_4 = conv2d_without_bias(rr_8_4, weights['wc_8_4'])\n",
    "    rcv_8_5 = conv2d_without_bias(rr_8_5, weights['wc_8_5'])\n",
    "    rcv_8_6 = conv2d_without_bias(rr_8_6, weights['wc_8_6'])\n",
    "    rcv_8_7 = conv2d_without_bias(rr_8_7, weights['wc_8_7'])\n",
    "    rcv_8_8 = conv2d_without_bias(rr_8_8, weights['wc_8_8'])\n",
    "    \n",
    "#     #REGION LAYER OUTPUT\n",
    "    rout_1_1 = tf.add(r_1_1, rcv_1_1)\n",
    "    rout_1_2 = tf.add(r_1_2, rcv_1_2)\n",
    "    rout_1_3 = tf.add(r_1_3, rcv_1_3)\n",
    "    rout_1_4 = tf.add(r_1_4, rcv_1_4)\n",
    "    rout_1_5 = tf.add(r_1_5, rcv_1_5)\n",
    "    rout_1_6 = tf.add(r_1_6, rcv_1_6)\n",
    "    rout_1_7 = tf.add(r_1_7, rcv_1_7)\n",
    "    rout_1_8 = tf.add(r_1_8, rcv_1_8)\n",
    "    rout_2_1 = tf.add(r_2_1, rcv_2_1)\n",
    "    rout_2_2 = tf.add(r_2_2, rcv_2_2)\n",
    "    rout_2_3 = tf.add(r_2_3, rcv_2_3)\n",
    "    rout_2_4 = tf.add(r_2_4, rcv_2_4)\n",
    "    rout_2_5 = tf.add(r_2_5, rcv_2_5)\n",
    "    rout_2_6 = tf.add(r_2_6, rcv_2_6)\n",
    "    rout_2_7 = tf.add(r_2_7, rcv_2_7)\n",
    "    rout_2_8 = tf.add(r_2_8, rcv_2_8)\n",
    "    rout_3_1 = tf.add(r_3_1, rcv_3_1)\n",
    "    rout_3_2 = tf.add(r_3_2, rcv_3_2)\n",
    "    rout_3_3 = tf.add(r_3_3, rcv_3_3)\n",
    "    rout_3_4 = tf.add(r_3_4, rcv_3_4)\n",
    "    rout_3_5 = tf.add(r_3_5, rcv_3_5)\n",
    "    rout_3_6 = tf.add(r_3_6, rcv_3_6)\n",
    "    rout_3_7 = tf.add(r_3_7, rcv_3_7)\n",
    "    rout_3_8 = tf.add(r_3_8, rcv_3_8)\n",
    "    rout_4_1 = tf.add(r_4_1, rcv_4_1)\n",
    "    rout_4_2 = tf.add(r_4_2, rcv_4_2)\n",
    "    rout_4_3 = tf.add(r_4_3, rcv_4_3)\n",
    "    rout_4_4 = tf.add(r_4_4, rcv_4_4)\n",
    "    rout_4_5 = tf.add(r_4_5, rcv_4_5)\n",
    "    rout_4_6 = tf.add(r_4_6, rcv_4_6)\n",
    "    rout_4_7 = tf.add(r_4_7, rcv_4_7)\n",
    "    rout_4_8 = tf.add(r_4_8, rcv_4_8)\n",
    "    rout_5_1 = tf.add(r_5_1, rcv_5_1)\n",
    "    rout_5_2 = tf.add(r_5_2, rcv_5_2)\n",
    "    rout_5_3 = tf.add(r_5_3, rcv_5_3)\n",
    "    rout_5_4 = tf.add(r_5_4, rcv_5_4)\n",
    "    rout_5_5 = tf.add(r_5_5, rcv_5_5)\n",
    "    rout_5_6 = tf.add(r_5_6, rcv_5_6)\n",
    "    rout_5_7 = tf.add(r_5_7, rcv_5_7)\n",
    "    rout_5_8 = tf.add(r_5_8, rcv_5_8)\n",
    "    rout_6_1 = tf.add(r_6_1, rcv_6_1)\n",
    "    rout_6_2 = tf.add(r_6_2, rcv_6_2)\n",
    "    rout_6_3 = tf.add(r_6_3, rcv_6_3)\n",
    "    rout_6_4 = tf.add(r_6_4, rcv_6_4)\n",
    "    rout_6_5 = tf.add(r_6_5, rcv_6_5)\n",
    "    rout_6_6 = tf.add(r_6_6, rcv_6_6)\n",
    "    rout_6_7 = tf.add(r_6_7, rcv_6_7)\n",
    "    rout_6_8 = tf.add(r_6_8, rcv_6_8)\n",
    "    rout_7_1 = tf.add(r_7_1, rcv_7_1)\n",
    "    rout_7_2 = tf.add(r_7_2, rcv_7_2)\n",
    "    rout_7_3 = tf.add(r_7_3, rcv_7_3)\n",
    "    rout_7_4 = tf.add(r_7_4, rcv_7_4)\n",
    "    rout_7_5 = tf.add(r_7_5, rcv_7_5)\n",
    "    rout_7_6 = tf.add(r_7_6, rcv_7_6)\n",
    "    rout_7_7 = tf.add(r_7_7, rcv_7_7)\n",
    "    rout_7_8 = tf.add(r_7_8, rcv_7_8)\n",
    "    rout_8_1 = tf.add(r_8_1, rcv_8_1)\n",
    "    rout_8_2 = tf.add(r_8_2, rcv_8_2)\n",
    "    rout_8_3 = tf.add(r_8_3, rcv_8_3)\n",
    "    rout_8_4 = tf.add(r_8_4, rcv_8_4)\n",
    "    rout_8_5 = tf.add(r_8_5, rcv_8_5)\n",
    "    rout_8_6 = tf.add(r_8_6, rcv_8_6)\n",
    "    rout_8_7 = tf.add(r_8_7, rcv_8_7)\n",
    "    rout_8_8 = tf.add(r_8_8, rcv_8_8)\n",
    "\n",
    "    print('rout_1_1')\n",
    "    print(rout_1_1)\n",
    "    r1 = tf.concat(2, [rout_1_1, rout_1_2, rout_1_3, rout_1_4, rout_1_5, rout_1_6, rout_1_7, rout_1_8])\n",
    "    r2 = tf.concat(2, [rout_2_1, rout_2_2, rout_2_3, rout_2_4, rout_2_5, rout_2_6, rout_2_7, rout_2_8])\n",
    "    r3 = tf.concat(2, [rout_3_1, rout_3_2, rout_3_3, rout_3_4, rout_3_5, rout_3_6, rout_3_7, rout_3_8])\n",
    "    r4 = tf.concat(2, [rout_4_1, rout_4_2, rout_4_3, rout_4_4, rout_4_5, rout_4_6, rout_4_7, rout_4_8])\n",
    "    r5 = tf.concat(2, [rout_5_1, rout_5_2, rout_5_3, rout_5_4, rout_5_5, rout_5_6, rout_5_7, rout_5_8])\n",
    "    r6 = tf.concat(2, [rout_6_1, rout_6_2, rout_6_3, rout_6_4, rout_6_5, rout_6_6, rout_6_7, rout_6_8])\n",
    "    r7 = tf.concat(2, [rout_7_1, rout_7_2, rout_7_3, rout_7_4, rout_7_5, rout_7_6, rout_7_7, rout_7_8])\n",
    "    r8 = tf.concat(2, [rout_8_1, rout_8_2, rout_8_3, rout_8_4, rout_8_5, rout_8_6, rout_8_7,rout_8_8])\n",
    "    \n",
    "    print('r1')\n",
    "    print(r1)\n",
    "    rtotal = tf.concat(1, [r1, r2, r3, r4, r5, r6, r7, r8])\n",
    "    \n",
    "    print('rtotal')\n",
    "    print(rtotal)\n",
    "    mpool = maxpool2d(rtotal, 2)\n",
    "    print('mpool')\n",
    "    print(mpool)\n",
    "    conv4 = conv2d(mpool, weights['wc4'], biases['bc4'])\n",
    "    conv4_relu = tf.nn.relu(conv4)\n",
    "    \n",
    "    conv5 = conv2d(conv4_relu, weights['wc5'], biases['bc5'])\n",
    "    conv5_relu = tf.nn.relu(conv5)\n",
    "    \n",
    "    conv6 = conv2d(conv5, weights['wc6'], biases['bc6'], 2)\n",
    "    conv6_relu = tf.nn.relu(conv6)\n",
    "\n",
    "    conv7 = conv2d(conv6_relu, weights['wc7'], biases['bc7'])\n",
    "    conv7_relu = tf.nn.relu(conv7)\n",
    "    \n",
    "    print(conv7)\n",
    "    #Fully connected layer fc8\n",
    "    fc8 = tf.reshape(conv7_relu, [-1, weights['w_fc8'].get_shape().as_list()[0]])\n",
    "    fc8 = tf.add(tf.matmul(fc8, weights['w_fc8']), biases['b_fc8'])\n",
    "    #fc8 = tf.nn.relu(fc8)\n",
    "    fc8 = tf.nn.dropout(fc8, dropout)\n",
    "    \n",
    "    print(fc8)\n",
    "    #Fully connected layer fc9\n",
    "    fc9 = tf.reshape(fc8, [-1, weights['w_fc9'].get_shape().as_list()[0]])\n",
    "    fc9 = tf.add(tf.matmul(fc9, weights['w_fc9']), biases['b_fc9'])\n",
    "    fc9 = tf.nn.relu(fc9)\n",
    "    fc9 = tf.nn.dropout(fc9, dropout)\n",
    "    \n",
    "    \n",
    "    print(fc9)\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc9, weights['w_out']), biases['b_out'])\n",
    "    print(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    \n",
    "    #11x11 conv, 3 input(rgb), 32 outputs, 1st layer\n",
    "    'wc1': tf.Variable(tf.random_normal([11, 11, 3, 32])),\n",
    "    #8x8 conv, 32 input, 16 outputs, conv4 layer\n",
    "    'wc4': tf.Variable(tf.random_normal([8, 8, 32, 16])),\n",
    "    #8x8 conv, 16 input, 16 outputs, conv5 layer\n",
    "    'wc5': tf.Variable(tf.random_normal([8, 8, 16, 16])),\n",
    "    #6x6 conv, 16 input, 16 outputs, conv6 layer\n",
    "    'wc6': tf.Variable(tf.random_normal([6, 6, 16, 16])),\n",
    "    #5x5 conv, 16 input, 16 outputs, conv7 layer\n",
    "    'wc7': tf.Variable(tf.random_normal([5, 5, 16, 16])),\n",
    "    \n",
    "    \n",
    "    # Region layer local convolution weights\n",
    "    'wc_1_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_1_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_2_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_3_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_4_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_5_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_6_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_7_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_1': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_3': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_4': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_5': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_6': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_7': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    'wc_8_8': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    \n",
    "    \n",
    "    # fully connected, 27*27*16 inputs, 4096 outputs\n",
    "    'w_fc8': tf.Variable(tf.random_normal([27*27*16, 4096])),\n",
    "    # fully connected, 4096 inputs, 2048 outputs\n",
    "    'w_fc9': tf.Variable(tf.random_normal([4096, 2048])),\n",
    "    # fully connected, 2048 inputs, 2 outputs\n",
    "    'w_out': tf.Variable(tf.random_normal([2048, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    #convolution layer biases\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc4': tf.Variable(tf.random_normal([16])),\n",
    "    'bc5': tf.Variable(tf.random_normal([16])),\n",
    "    'bc6': tf.Variable(tf.random_normal([16])),\n",
    "    'bc7': tf.Variable(tf.random_normal([16])),\n",
    "    \n",
    "    \n",
    "    #fully connected layers biases\n",
    "    'b_fc8': tf.Variable(tf.random_normal([4096])),\n",
    "    'b_fc9': tf.Variable(tf.random_normal([2048])),\n",
    "    'b_out': tf.Variable(tf.random_normal([2]))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#sigmoid_cross_entropy_with_logits\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "#flag for training or testing\n",
    "training = True\n",
    "\n",
    "\n",
    "sess  = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    # Launch the graph\n",
    "    # with tf.Session() as sess:\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    itera = 0\n",
    "    while itera < 3:\n",
    "        step = 1\n",
    "        # Keep training until reach max iterations\n",
    "        while step * batch_size < training_iters:\n",
    "            batch_x, batch_y  = get_next_batch((step-1)*batch_size, step*batch_size-1)\n",
    "\n",
    "            print(len(batch_x))\n",
    "            print(len(batch_y))\n",
    "            #logits and labels must be same size\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                           keep_prob: dropout})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                                  y: batch_y,\n",
    "                                                                  keep_prob: 1.})\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        \n",
    "        #if(itera == 2):\n",
    "        saver.save(sess, \"model_5_6_a.ckpt\", itera)\n",
    "        print(\"Optimization Finished!\")\n",
    "        itera = itera+1\n",
    "else:\n",
    "    #Testing\n",
    "    saver.restore(sess, \"model_5_6.ckpt-3\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "    \n",
    "    \n",
    "# for i in range(0,20):\n",
    "#     startn = 1200+i*10\n",
    "#     endn = startn+9\n",
    "#     print(startn, endn)\n",
    "#     batch_x, batch_y = get_next_batch(startn,endn)\n",
    "#     print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "    \n",
    "    \n",
    "    \n",
    "#Gradient computation\n",
    "# startn = 1200\n",
    "# endn = 1209\n",
    "# batch_x, batch_y = get_next_batch(startn,endn)\n",
    "# outp = tf.slice(pred, [0, 0], [10, 2])\n",
    "# #var_grad  = tf.gradients(outp, x)[0]\n",
    "# var_grad  = tf.gradients(pred, x)[0]\n",
    "# ans = sess.run(var_grad, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "# ansx = tf.reshape(ans[0], shape=[-1, 170, 170, 3])\n",
    "# #print(sess.run(outp, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "\n",
    "\n",
    "# #print(ansx.eval())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# anspx = sess.run(ansx)\n",
    "# anspx1 = anspx[0,:,:,0]\n",
    "# anspx2 = anspx[0,:,:,1]\n",
    "# anspx3 = anspx[0,:,:,2]\n",
    "# print(anspx1.shape)\n",
    "# import numpy\n",
    "# numpy.savetxt(\"anspx1_26_6\", anspx1, delimiter=\",\")\n",
    "# numpy.savetxt(\"anspx2_26_6\", anspx2, delimiter=\",\")\n",
    "# numpy.savetxt(\"anspx3_26_6\", anspx3, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1209)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1210, 1219)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1220, 1229)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1230, 1239)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1240, 1249)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1250, 1259)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1260, 1269)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1270, 1279)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1280, 1289)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1290, 1299)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1300, 1309)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1310, 1319)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1320, 1329)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1330, 1339)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1340, 1349)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1350, 1359)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1360, 1369)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1370, 1379)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1380, 1389)\n",
      "('Testing Accuracy:', 1.0)\n",
      "(1390, 1399)\n",
      "('Testing Accuracy:', 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    startn = 1200+i*10\n",
    "    endn = startn+9\n",
    "    print(startn, endn)\n",
    "    batch_x, batch_y = get_next_batch(startn,endn)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 170)\n"
     ]
    }
   ],
   "source": [
    "#Gradient computation\n",
    "startn = 1200\n",
    "endn = 1209\n",
    "batch_x, batch_y = get_next_batch(startn,endn)\n",
    "outp = tf.slice(pred, [0, 0], [10, 2])\n",
    "var_grad  = tf.gradients(outp, x)[0]\n",
    "#var_grad  = tf.gradients(pred, x)[0]\n",
    "ans = sess.run(var_grad, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "ansx = tf.reshape(ans[0], shape=[-1, 170, 170, 3])\n",
    "#print(sess.run(outp, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "anspx = sess.run(ansx)\n",
    "anspx1 = anspx[0,:,:,0]\n",
    "anspx2 = anspx[0,:,:,1]\n",
    "anspx3 = anspx[0,:,:,2]\n",
    "print(anspx1.shape)\n",
    "import numpy\n",
    "numpy.savetxt(\"anspx1_26_6\", anspx1, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx2_26_6\", anspx2, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx3_26_6\", anspx3, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 170)\n"
     ]
    }
   ],
   "source": [
    "startn = 1200\n",
    "endn = 1209\n",
    "batch_x, batch_y = get_next_batch(startn,endn)\n",
    "outp = tf.slice(pred, [0, 0], [10, 2])\n",
    "#var_grad  = tf.gradients(outp, x)[0]\n",
    "var_grad  = tf.gradients(pred, x)[0]\n",
    "ans = sess.run(var_grad, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "ansx = tf.reshape(ans[0], shape=[-1, 170, 170, 3])\n",
    "#print(sess.run(outp, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "\n",
    "\n",
    "#print(ansx.eval())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "anspx = sess.run(ansx)\n",
    "anspx1 = anspx[0,:,:,0]\n",
    "anspx2 = anspx[0,:,:,1]\n",
    "anspx3 = anspx[0,:,:,2]\n",
    "print(anspx1.shape)\n",
    "import numpy\n",
    "numpy.savetxt(\"anspx1_17_14\", anspx1, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx2_17_14\", anspx2, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx3_17_14\", anspx3, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-30d74f724129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "outp = tf.slice(pred, [0, 0], [10, 2])\n",
    "print(sess.run(outp, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Tensor name \"beta1_power_1\" not found in checkpoint files model_15_5.ckpt-2\n\t [[Node: save_4/restore_slice_1121 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/restore_slice_1121/tensor_name, save_4/restore_slice_1121/shape_and_slice)]]\nCaused by op u'save_4/restore_slice_1121', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b2a18660c919>\", line 541, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 502, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 256, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 171, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 196, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c9daa8450c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_15_5.ckpt-2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pranjal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1088\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restore called with invalid save path %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1090\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m       raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[1;32m--> 659\u001b[1;33m                                             e.code)\n\u001b[0m\u001b[0;32m    660\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Tensor name \"beta1_power_1\" not found in checkpoint files model_15_5.ckpt-2\n\t [[Node: save_4/restore_slice_1121 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_4/Const_0, save_4/restore_slice_1121/tensor_name, save_4/restore_slice_1121/shape_and_slice)]]\nCaused by op u'save_4/restore_slice_1121', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-b2a18660c919>\", line 541, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 502, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 256, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 171, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 196, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, \"model_15_5.ckpt-2\")\n",
    "print('pranjal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using eval(): No default session is registered. Use `with sess.as_default()` or pass an explicit session to eval(session=sess)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-62693f3167d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mansx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m170\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m170\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mansx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \"\"\"\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3318\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3320\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using eval(): No default \"\n\u001b[0m\u001b[0;32m   3321\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3322\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using eval(): No default session is registered. Use `with sess.as_default()` or pass an explicit session to eval(session=sess)"
     ]
    }
   ],
   "source": [
    "startn = 1200\n",
    "endn = 1209\n",
    "batch_x, batch_y = get_next_batch(startn,endn)\n",
    "#batch_x = tf.reshape(batch_x, shape=[-1, 170, 170, 3])\n",
    "var_grad  = tf.gradients(pred, x)[0]\n",
    "#var_gradp = tf.reshape(var_grad, shape=[-1, 170, 170, 3])[0]\n",
    "ans = sess.run(var_grad, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "ansx = tf.reshape(ans[0], shape=[-1, 170, 170, 3])\n",
    "print(ansx.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ansx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-11972990526e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manspx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mansx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ansx' is not defined"
     ]
    }
   ],
   "source": [
    "anspx = sess.run(ansx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 170)\n"
     ]
    }
   ],
   "source": [
    "anspx1 = anspx[0,:,:,0]\n",
    "anspx2 = anspx[0,:,:,1]\n",
    "anspx3 = anspx[0,:,:,2]\n",
    "print(anspx1.shape)\n",
    "import numpy\n",
    "numpy.savetxt(\"anspx1\", anspx1, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx2\", anspx2, delimiter=\",\")\n",
    "numpy.savetxt(\"anspx3\", anspx3, delimiter=\",\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-538aa3e921e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manspx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "img = mpimg.fromarray(anspx[0, :, :, :], 'RGB')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1209)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10, 86700) for Tensor u'Reshape_8:0', which has shape '(1, 170, 170, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a12a187868dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mendn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    554\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (10, 86700) for Tensor u'Reshape_8:0', which has shape '(1, 170, 170, 3)'"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    startn = 1200+i*10\n",
    "    endn = startn+9\n",
    "    print(startn, endn)\n",
    "    batch_x, batch_y = get_next_batch(startn,endn)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_8:0\", shape=(1, 170, 170, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = get_next_batch(2501, 2511)\n",
    "print(len(batch_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1209)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_next_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d1f71405da8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mendn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstartn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mendn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_next_batch' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    startn = 1200+i*10\n",
    "    endn = startn+9\n",
    "    print(startn, endn)\n",
    "    batch_x, batch_y = get_next_batch(startn,endn)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inputa = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(inputa[0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
